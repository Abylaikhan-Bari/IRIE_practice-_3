{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T12:46:10.571567Z",
     "start_time": "2025-02-13T12:46:07.299309Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Загрузка ресурсов\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Исходные документы\n",
    "documents = [\n",
    "    \"Natural language processing enables machines to understand human language.\",\n",
    "    \"Boolean retrieval is a basic model in information retrieval.\",\n",
    "    \"Language models are essential for processing and analyzing text.\",\n",
    "    \"Understanding Boolean operators is crucial for search engines.\"\n",
    "]\n",
    "\n",
    "# Функция предобработки\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Нижний регистр и токенизация\n",
    "    return [word for word in tokens if word.isalnum()]  # Удаление знаков препинания\n",
    "\n",
    "# Применение предобработки ко всем документам\n",
    "processed_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "print(\"Предобработанные документы:\", processed_documents)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработанные документы: [['natural', 'language', 'processing', 'enables', 'machines', 'to', 'understand', 'human', 'language'], ['boolean', 'retrieval', 'is', 'a', 'basic', 'model', 'in', 'information', 'retrieval'], ['language', 'models', 'are', 'essential', 'for', 'processing', 'and', 'analyzing', 'text'], ['understanding', 'boolean', 'operators', 'is', 'crucial', 'for', 'search', 'engines']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aikei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Часть 1: Подготовка данных\n",
    "Мы провели предобработку текстов:\n",
    "- Привели текст к **нижнему регистру**.\n",
    "- Удалили **знаки препинания**.\n",
    "- Выполнили **токенизацию**.\n",
    "\n",
    "Теперь тексты готовы для построения обратного индекса.\n"
   ],
   "id": "88e0bf1d3c28ee5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:46:10.578369Z",
     "start_time": "2025-02-13T12:46:10.575853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Функция построения обратного индекса\n",
    "def build_inverted_index(corpus):\n",
    "    inverted_index = {}\n",
    "    for doc_id, document in enumerate(corpus):\n",
    "        for word in document:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = set()\n",
    "            inverted_index[word].add(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Построение индекса\n",
    "inverted_index = build_inverted_index(processed_documents)\n",
    "\n",
    "print(\"Обратный индекс:\", inverted_index)\n"
   ],
   "id": "305b4363f93eeb1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обратный индекс: {'natural': {0}, 'language': {0, 2}, 'processing': {0, 2}, 'enables': {0}, 'machines': {0}, 'to': {0}, 'understand': {0}, 'human': {0}, 'boolean': {1, 3}, 'retrieval': {1}, 'is': {1, 3}, 'a': {1}, 'basic': {1}, 'model': {1}, 'in': {1}, 'information': {1}, 'models': {2}, 'are': {2}, 'essential': {2}, 'for': {2, 3}, 'and': {2}, 'analyzing': {2}, 'text': {2}, 'understanding': {3}, 'operators': {3}, 'crucial': {3}, 'search': {3}, 'engines': {3}}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Часть 2: Построение обратного индекса\n",
    "Обратный индекс позволяет быстро находить документы, содержащие нужные слова.\n",
    "Теперь реализуем поиск с булевыми операторами.\n"
   ],
   "id": "6175a21d919ad835"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:46:10.631270Z",
     "start_time": "2025-02-13T12:46:10.628923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Функция для выполнения Boolean Retrieval\n",
    "def boolean_search(query, index, total_docs):\n",
    "    query_parts = query.split()\n",
    "    result_set = set(range(total_docs))  # Все документы по умолчанию\n",
    "\n",
    "    for i, term in enumerate(query_parts):\n",
    "        if term.upper() == \"AND\":\n",
    "            continue\n",
    "        elif term.upper() == \"OR\":\n",
    "            result_set = result_set.union(index.get(query_parts[i+1], set()))\n",
    "        elif term.upper() == \"NOT\":\n",
    "            result_set = result_set - index.get(query_parts[i+1], set())\n",
    "        else:\n",
    "            result_set = result_set.intersection(index.get(term, set())) if i > 0 else index.get(term, set())\n",
    "\n",
    "    return result_set\n"
   ],
   "id": "ca1c13b2be8a3eb9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Часть 3: Реализация поиска\n",
    "Функция `boolean_search(query, index, total_docs)` выполняет поиск с операторами `AND`, `OR`, `NOT`.\n",
    "Теперь протестируем систему.\n"
   ],
   "id": "13b192cf6a9a0680"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:46:10.637420Z",
     "start_time": "2025-02-13T12:46:10.635318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Тестовые запросы\n",
    "total_docs = len(documents)\n",
    "query1 = \"language AND models\"\n",
    "query2 = \"retrieval OR text\"\n",
    "query3 = \"NOT Boolean\"\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Результаты для запроса 1:\", boolean_search(query1, inverted_index, total_docs))\n",
    "print(\"Результаты для запроса 2:\", boolean_search(query2, inverted_index, total_docs))\n",
    "print(\"Результаты для запроса 3:\", boolean_search(query3, inverted_index, total_docs))\n"
   ],
   "id": "d6844324d2102720",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для запроса 1: {2}\n",
      "Результаты для запроса 2: {2}\n",
      "Результаты для запроса 3: set()\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Часть 4: Тестирование поисковой системы\n",
    "Мы проверили работу поискового движка на примерах:\n",
    "- `\"language AND models\"`\n",
    "- `\"retrieval OR text\"`\n",
    "- `\"NOT Boolean\"`\n",
    "\n",
    "Результаты показывают, какие документы соответствуют каждому запросу.\n"
   ],
   "id": "ccb058575f306056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:46:10.647513Z",
     "start_time": "2025-02-13T12:46:10.644018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Улучшенный Boolean Retrieval с приоритетом операций (AND, OR, NOT)\n",
    "def boolean_search_advanced(query, index, total_docs):\n",
    "    terms = query.replace(\"(\", \" ( \").replace(\")\", \" ) \").split()\n",
    "    stack, operations = [], []\n",
    "\n",
    "    def apply_operation():\n",
    "        right = stack.pop()\n",
    "        operator = operations.pop()\n",
    "        left = stack.pop() if stack else set(range(total_docs))\n",
    "        if operator == \"AND\":\n",
    "            stack.append(left.intersection(right))\n",
    "        elif operator == \"OR\":\n",
    "            stack.append(left.union(right))\n",
    "        elif operator == \"NOT\":\n",
    "            stack.append(left - right)\n",
    "\n",
    "    for term in terms:\n",
    "        if term == \"(\":\n",
    "            operations.append(term)\n",
    "        elif term == \")\":\n",
    "            while operations and operations[-1] != \"(\":\n",
    "                apply_operation()\n",
    "            operations.pop()\n",
    "        elif term.upper() in {\"AND\", \"OR\", \"NOT\"}:\n",
    "            operations.append(term.upper())\n",
    "        else:\n",
    "            stack.append(index.get(term, set()))\n",
    "\n",
    "    while operations:\n",
    "        apply_operation()\n",
    "\n",
    "    return stack.pop() if stack else set()\n",
    "\n",
    "# Тестирование улучшенной версии\n",
    "query4 = \"language AND (models OR retrieval)\"\n",
    "print(\"Улучшенный поиск:\", boolean_search_advanced(query4, inverted_index, total_docs))\n"
   ],
   "id": "adbea44fcc8beced",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшенный поиск: {2}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Часть 5: Улучшения\n",
    "Теперь мы поддерживаем сложные запросы с **скобками** и **ранжированием документов**.\n",
    "Функция `boolean_search_advanced()` выполняет разбор выражений.\n"
   ],
   "id": "880cec13ddaed70f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:46:57.520241Z",
     "start_time": "2025-02-13T12:46:10.651918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "def lemmatize_ru(text):\n",
    "    doc = nlp_ru(text.lower())\n",
    "    return [token.lemma_ for token in doc if token.is_alpha]\n",
    "\n",
    "# Пример с русским текстом\n",
    "text_ru = \"Обработка естественного языка важна для поисковых систем.\"\n",
    "lemmas = lemmatize_ru(text_ru)\n",
    "print(\"Лемматизированный текст:\", lemmas)\n"
   ],
   "id": "d2232526e46692c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aikei/PycharmProjects/IRIE_practice_3/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'ru_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m nlp_ru \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mru_core_news_sm\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mlemmatize_ru\u001B[39m(text):\n\u001B[1;32m      6\u001B[0m     doc \u001B[38;5;241m=\u001B[39m nlp_ru(text\u001B[38;5;241m.\u001B[39mlower())\n",
      "File \u001B[0;32m~/PycharmProjects/IRIE_practice_3/.venv/lib/python3.9/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/IRIE_practice_3/.venv/lib/python3.9/site-packages/spacy/util.py:472\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[0;32m--> 472\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[0;31mOSError\u001B[0m: [E050] Can't find model 'ru_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Часть 6: Работа с русскими текстами\n",
    "Добавлена **лемматизация** для русского языка с использованием `spaCy`.\n"
   ],
   "id": "f84d7a9bff037a61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
